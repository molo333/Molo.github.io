<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="prml EM"><meta name="keywords" content=""><meta name="author" content="molo"><meta name="copyright" content="molo"><title>prml EM | Molo</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.3.0'
} </script><meta name="generator" content="Hexo 6.3.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#EM%E7%AE%97%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">EM算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">高斯混合模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#k%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB"><span class="toc-number">2.1.</span> <span class="toc-text">k均值聚类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%AF%E5%8A%AA%E5%88%A9%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">伯努利混合模型</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/p/2.jpg"></div><div class="author-info__name text-center">molo</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">5</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">一些大佬</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://hammer-wh.github.io/">锤子</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://sikouhjw.github.io/">何神我的何神</a></div></div></div><div id="content-outer"><div class="plain" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Molo</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right"></span></div></div><div class="layout" id="content-inner"><article id="post"><div class="plain" id="post-title">prml EM</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-06-01</time></div><div class="article-container" id="post-content"><p>prml第九章<br><span id="more"></span></p>
<h2 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h2><p>期望最大化（EM）算法，是寻找具有潜在变量的概率模型的最大似然解的一种通用方法。<br>我们希望能对观测变量进行建模，但是我们一般会遇到无法对观测变量直接建模，需要引入隐含变量，用含有隐藏变量的分布来建模。比如说高斯分布是单峰的，虽然很灵活但对多峰分布等情况不能很好的近似，有很大的局限性。但是引入潜在变量变成混合高斯就可以解决他的局限性。考虑一个概率模型有观测变量X和隐含变量Z。他们联合概率分布$p(x,z \vert \theta)$由参数$\theta$控制。我们目标是找一组参数$\theta$使观测到的结果$x$最大。就是要最大化对数似然函数$\ln p(x\vert \theta)$。由于隐含变量Z的存在，我们关于$\theta$最优化$p(x\vert \theta)$一般很困难(涉及到对数里面有求和)：</p>
<script type="math/tex; mode=display">\ln p(x\vert \theta) = \ln \int_z p(x,z \vert \theta) dz</script><p>我们引入潜在变量分$q(x)$和Jensen不等式（见prml1.2）lnx是上凸函数有$f(E(x)) \geq E(f(x))$。</p>
<script type="math/tex; mode=display">\begin{aligned}
\ln p(x\vert \theta) &= \ln \int_z p(x,z \vert \theta)dz\\&=
\ln \int_z q(z)\frac{p(x,z \vert \theta)}{q(z)}dz
\\& = \ln (E_{q(z)}[\frac{p(x,z \vert \theta)}{q(z)}])
\\ &  \geq  E_{q(z)}[\ln \frac{p(x,z \vert \theta)}{q(z)}] =ELBO
\end{aligned}</script><p>ELBO是证据下界（Evidence Lower Bound）。<br>当$\frac{p(x,z \vert \theta)}{q(z)}$是常数时等式成立。</p>
<script type="math/tex; mode=display">\begin{aligned}
c &= \frac{p(x,z \vert \theta)}{q(z)}
\\ \int_zcq(z)dz &=  \int_zp(x,z\vert \theta)dz
\\c &= p(x \vert\theta)\\
q(z) &=  \frac{p(x,z \vert \theta)}{p(x \vert\theta)} = p(z \vert x,\theta)
\end{aligned}</script><p>即得到狭义的EM算法：<br>选择$q(z)$使下界$ELBO$等于对数似然函数$\ln p(x \vert \theta)$，然后关于$\theta$最大化下界ELBO。得到的新的$\theta$对应的ELBO就能大于旧的$\theta$对应的ELBO，这样新的对数似然$\ln p(x\vert \theta)$大于旧的$\ln p(x\vert \theta)$。从而使对数似然函数变大。<br><img src="/2023/06/01/prml%20EM/1.png" alt><br>E步骤：<br>找到$q(z) = p(z \vert x,\theta^*)$<br>M步骤：</p>
<script type="math/tex; mode=display">\theta^* = \arg\max_{\theta} (E_{q(z)}[\ln \frac{p(x,z \vert \theta)}{q(z)}])</script><p>考虑以下分解方法(引入潜在变量分布q(x)，想办法凑出上面我们得到的下界):</p>
<script type="math/tex; mode=display">\begin{aligned}
    \ln p(x\vert \theta) &= \ln p(x,z \vert \theta) - \ln p(z \vert x, \theta)\\
    & = \ln \frac{p(x,z \vert \theta)}{q(z)} - \ln \frac{p(z \vert x, \theta)}{q(z)}\\
    & = E_{q(z)}[\ln \frac{p(x,z \vert \theta)}{q(z)}]  - E_{q(z)}[\ln \frac{p(z \vert x, \theta)}{q(z)}]\\
    & = ELBO  + KL(q \vert \vert p)
\end{aligned}</script><p>$KL()$为两个函数之间的KL散度，用来描述两个函数之间的相似性，KL散度一定大于0。（KL散度描述见prml1.2）<br>相当于ELBO和对数似然之间的距离是$q(z)$和$p(z \vert x, \theta)$之间的KL散度。即对数似然等于ELBO加KL散度。<br>这样可以得到一般化的EM算法。与狭义的E步骤不同，当我们无法得到$p(z \vert x, \theta)$时，我们可以令已知的分布$q(z)$去逼近$p(z \vert x, \theta)$。即令KL散度最小。<br>E步骤：</p>
<script type="math/tex; mode=display">q =  \arg\min_{q}KL(q \vert \vert p)</script><p>M步骤：</p>
<script type="math/tex; mode=display">\theta^* = \arg\max_{\theta} (E_{q(z)}[\ln \frac{p(x,z \vert \theta)}{q(z)}])</script><h2 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h2><p>高斯混合模型是将多个的高斯分布进行线性叠加的概率模型。他认为得到的数据分为好多类，每一类服从特定均值和方差的高斯分布。他能更好的描述多峰的分布。<br><img src="/2023/06/01/prml%20EM/2.png" alt><br>上图可以看到，数据集($x_1,x_2…x_n…x_N$)（绿色数据点）主要聚集成两类，混合高斯模型可以很好的拟合。<br>假设我们有数据集($x_1,x_2…x_n…x_N$)，引入各聚类的均值$\mu_k$和方差$\Sigma_k$。高斯混合模型的边缘概率密度：</p>
<script type="math/tex; mode=display">
p(x_n \vert \pi,\mu,\Sigma) = \sum_{k = 1}^K \pi_k N(x_n \vert \mu_k,\Sigma_k)</script><p>引入n个K维的二值随机隐含变量$z_n$，每一个$z_n$中只有一个特殊的$z_{nk}$等于1其余为0。他来描述第n个数据被分为哪一类，若$z_{nk}= 1$则说明第n个数据$\mathbf x_n$属于第k类，$z_n$服从多项式分布。</p>
<script type="math/tex; mode=display">\begin{aligned}
    &p(z_{nk} = 1) = \pi_k &\sum_k^K \pi_k = 1
\end{aligned}</script><p>$z_{nk} = 1$时第n个数据点数据点$x_n$被分为k类，服从$\mu_k,\Sigma_k$的高斯分布。就是说似然函数是：</p>
<script type="math/tex; mode=display">p(x_n \vert z_{nk} = 1) = N(x_n \vert \mu_k,\Sigma_k)</script><p>那么我们可以求到$z$的后验分布：</p>
<script type="math/tex; mode=display">\begin{aligned}
    p(z_{nk} = 1 \vert x_n) &= \frac{p(x_n \vert z_{nk} = 1)p(z_{nk} = 1)}{p(x_n)}\\
    & = \frac{\pi_kN(x_n \vert \mu_k,\Sigma_k)}{\sum_{j = 1}^K \pi_j N(x_n \vert \mu_j,\Sigma_j)}
\end{aligned}</script><p>多维高斯分布有：<br><img src="/2023/06/01/prml%20EM/3.png" alt><br>完整对数似然函数是：</p>
<script type="math/tex; mode=display">\begin{aligned}
     \ln p(\mathbf{x},\mathbf{z} \vert \pi,\mu,\Sigma) &= \ln \prod_{n=1}^N \prod_{k=1}^K (p(x_n \vert z_{nk} )p(z_{nk}))^{z_{nk}}\\& 
     = \sum_{n=1}^N\sum_{k=1}^K z_{nk} (\ln \pi_k N(x_n \vert \mu_k,\Sigma_k))\\& = \sum_{n=1}^N\sum_{k=1}^K z_{nk} (\ln \pi_k  -\frac{1}{2}(x_n - \mu_k)^T\Sigma_k^{-1}(x_n - \mu_k)-\frac{1}{2} \ln \vert \Sigma_k \vert) +C
\end{aligned}</script><p>C是与参数无关的常数；ELBO为：</p>
<script type="math/tex; mode=display">\begin{aligned}
    &E_{q(z)}[\sum_{n=1}^N\sum_{k=1}^K z_{nk} (\ln \pi_k  -\frac{1}{2}(x_n - \mu_k)^T\Sigma_k^{-1}(x_n - \mu_k)-\frac{1}{2} \ln \vert \Sigma_k \vert) + C ]
\\ & = \sum_{n=1}^N\sum_{k=1}^K q(z_{nk} = 1) (\ln \pi_k  -\frac{1}{2}(x_n - \mu_k)^T\Sigma_k^{-1}(x_n - \mu_k)-\frac{1}{2} \ln \vert \Sigma_k \vert) + C
\end{aligned}</script><p>对各参数（$\mu_k,\Sigma_k$）求导:<br>对$\mu_k,\Sigma_k$求导会遇到二项式求导的问题：</p>
<script type="math/tex; mode=display">
\alpha = X^T A X = \sum^n_{j}\sum^n_{i}a_{ij}x_ix_j</script><script type="math/tex; mode=display">
\frac{\partial\alpha}{\partial x_k} = \sum_{i \neq k}^n a_{ik}x_i + \sum_{j \neq k}^n a_{kj}x_j  + 2*a_{kk}x_k = \sum_{i}^n a_{ik}x_i + \sum_{j}^n a_{kj}x_j</script><script type="math/tex; mode=display">
\frac{\partial(X^T A X)}{X} = A^T X + A X</script><p>用上面得到的结论：</p>
<script type="math/tex; mode=display">
\mu_k = \frac{\sum_{n= 1}^N q(z_{nk} = 1)*x_n}{\sum_{n= 1}^N q(z_{nk} = 1)} = \frac{1}{N_k} [q(z_{nk} = 1)*x_n]</script><p>$N_k = \sum_{n= 1}^N q(z_{nk} = 1)$；对$\Sigma_k$求导时，会遇到对行列式求导的问题：<br>求矩阵行列式</p>
<script type="math/tex; mode=display">\vert A\vert = A_{ij}C_{ij}</script><p>$C_{ij}$是矩阵$A$元素$A_{ij}$的代数余子式；我们知道$A$的伴随矩阵是由$A$各元素的代数余子式构成的。即：</p>
<script type="math/tex; mode=display">A^{*}_{ij} = C_{ji}</script><p>所以有：</p>
<script type="math/tex; mode=display">\frac{\partial \vert A \vert}{\partial A_{ij}} = A^*_{ji}</script><p>即</p>
<script type="math/tex; mode=display">\frac{\partial \vert A \vert}{\partial A} = (A^*)^T</script><p>若$A$可逆有$A^* = \vert A \vert A^{-1}$</p>
<script type="math/tex; mode=display">{(\ln \vert A \vert)}^\prime = (A^{-1})^T</script><p>同时还会遇到以下导数:</p>
<script type="math/tex; mode=display">\frac{\partial (X^T A^{-1}X)}{\partial A}</script><p>要求这个需要先引入对$A^{-1}$的导数：</p>
<script type="math/tex; mode=display">\begin{aligned}
    0 &= \frac{\partial I}{\partial x} = \frac{\partial (AA^{-1})}{\partial x}\\
    0 &= A \frac{\partial (A^{-1})}{\partial x} + A^{-1}\frac{\partial A}{\partial x}\\
    \frac{\partial (A^{-1})}{\partial x} &= -A^{-1}\frac{\partial A}{\partial x}A^{-1}
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
    \frac{\partial (X^T A^{-1}X)}{\partial a_{ij}} &= -X^TA^{-1}\frac{\partial A}{\partial a_{ij}}A^{-1}X = X^TA^{-1}I_{ij}A^{-1}X \\ &= (X^T A^{-1}_{:i})(A^{-1}_{:j}X)
\end{aligned}</script><p>$I_{ij}$是一个n*n矩阵只有第$i$行第$j$列是1，其余位置是0。可以看图理解：<br><img src="/2023/06/01/prml%20EM/4.png" alt><br>然后我们要扩展成对矩阵求导（将上述两向量交换相乘顺序，既可以得到导数矩阵的转置）：</p>
<script type="math/tex; mode=display">
\frac{\partial (X^T A^{-1}X)}{\partial A} = (A^{-1}XX^TA^{-1})^T = (A^{-1})^TXX^T(A^{-1})^T</script><p>运用上述结论，即可以得到：</p>
<script type="math/tex; mode=display">\begin{aligned}
    &\sum_{n=1}^N q(z_{nk} = 1)((\Sigma_k^{-1})^T(x_n-\mu_k)(x_n-\mu_k)^T(\Sigma_k^{-1})^T - (\Sigma_k^{-1})^T) = 0\\
    &\Sigma_k = \frac{\sum_{n=1}^N q(z_{nk} = 1)(x_n-\mu_k)(x_n-\mu_k)^T}{N_k}
\end{aligned}</script><p>对$\pi_k$最大化需引入$\sum_k \pi_k = 1$的限制条件，用拉格朗日乘数法进行优化。</p>
<script type="math/tex; mode=display">\sum_n^{N}\frac{q(z_{nk} = 1)}{\pi_k} + \lambda  = 0</script><script type="math/tex; mode=display">\sum_k \pi_k = 1</script><script type="math/tex; mode=display">\lambda =  - \sum_k^K\sum_n^{N}q(z_{nk} = 1) = -N</script><script type="math/tex; mode=display">\pi_k = \frac{N_K}{N}</script><p>其中有$N = \sum_k^K\sum_n^{N}q(z_{nk} = 1)$。<br>这样我们就可以得到EM算法用于高斯混合模型的具体流程：<br>E步骤：<br>计算旧参数对应的后验分布q:</p>
<script type="math/tex; mode=display">q(z_{nk} = 1) = p(z_{nk} = 1 \vert x_n) = 
     \frac{\pi_kN(x_n \vert \mu_k,\Sigma_k)}{\sum_{j = 1}^K \pi_j N(x_n \vert \mu_j,\Sigma_j)}</script><p>M步骤:<br>更新参数：</p>
<script type="math/tex; mode=display">
\mu_k = \frac{1}{N_k} q(z_{nk} = 1)*x_n</script><script type="math/tex; mode=display">\Sigma_k = \frac{\sum_{n=1}^N q(z_{nk} = 1)(x_n-\mu_k)(x_n-\mu_k)^T}{N_k}</script><script type="math/tex; mode=display">\pi_k = \frac{N_K}{N}</script><p>其中：$N = \sum_k^K\sum_n^{N}q(z_{nk} = 1)$，$N_k = \sum_{n= 1}^N q(z_{nk} = 1)$<br>笼统地说，这个具体流程像是先确定几个中心点，然后第一步先求各个数据点属于每一类的概率，再以这个概率为权重求均值和方差，让数据点来更改中心点，就这样一步步迭代最后给各数据点聚类（这里的聚类用概率来表示，不是说这个数据点属于哪一类，而是说数据点属于各类的概率，是一种”软”聚类。），并得到与数据点拟合的高斯混合概率模型。</p>
<h3 id="k均值聚类"><a href="#k均值聚类" class="headerlink" title="k均值聚类"></a>k均值聚类</h3><p>假设有数据集($x_1,x_2…x_n…x_N$)，同时引入k个聚类中心$\mu_k$,k均值聚类目的是令以下J最小：</p>
<script type="math/tex; mode=display">J = r_{nk}*\vert\vert x_n - \mu_k \vert\vert^2</script><p>$r_{nk}$是分类的指示器，$r_{nk} = 1$证明数据$x_n$属于$k$类。<br>他的迭代方法也类似与EM算法两部分，只是他的后验概率比较特殊。<br>E步骤：</p>
<script type="math/tex; mode=display">r_{nk} = 1 (k = \arg\min_k \vert\vert x_n - \mu_k \vert\vert^2)</script><p>M步骤：<br>更新参数<script type="math/tex">\mu_k = \frac{\sum_n r_{nk} * x_n}{\sum_n r_{nk}}</script><br>K均值聚类其实是一种特殊的高斯混合模型。他将方差矩阵固定设置为$\epsilon I$，并让$\epsilon \to 0$。这样后验分布就变成：</p>
<script type="math/tex; mode=display">
    p(z_{nk} = 1 \vert x_n)  = \frac{\pi_k \exp(-\frac{\vert\vert x_n - \mu_k \vert\vert^2}{2\epsilon})}{\sum_{j = 1}^K \pi_j \exp(-\frac{\vert\vert x_n - \mu_j \vert\vert^2}{2\epsilon})} = r_{nk}</script><p>$\epsilon \to 0$时，相当于给$\vert\vert x_n - \mu_k \vert\vert^2$乘上一个很大的值。假如说$\vert\vert x_n - \mu_j \vert\vert^2 &gt; \vert\vert x_n - \mu_k \vert\vert^2$，稍微大一点就会被$\epsilon$扩大到大很多。然后再求负指数，负指数内值越大，得到的指数就越小。这样$\exp(-\frac{\vert\vert x_n - \mu_j \vert\vert^2}{2\epsilon}) &lt;&lt;\exp(-\frac{\vert\vert x_n - \mu_k \vert\vert^2}{2\epsilon})$。相当于只要$\vert\vert x_n - \mu_k \vert\vert^2$不是最小的，他就会给放的很小直到可以忽略，只有最小的可以留下。这样只有$k = \arg\min_k \vert\vert x_n - \mu_k \vert\vert^2$可以令$p(z_{nk} = 1 \vert x_n) = 1$即第二个等号成立。他的后验概率只能取0和1，相当于每一个样本只可能是某一类。是一种“硬”分类。同时只估计了均值而没有更新方差。</p>
<h2 id="伯努利混合模型"><a href="#伯努利混合模型" class="headerlink" title="伯努利混合模型"></a>伯努利混合模型</h2><p>我们考虑一个伯努利分布的混合模型，这种模型也被称为潜在类别分析。数据$\mathbf{x}$是有D个二值变量$x_i$组成的向量，变量$x_i$服从参数为$\mu_i$伯努利分布。即：<br><img src="/2023/06/01/prml%20EM/5.png" alt><br>然后我们考虑这种分布的有限混合：<br><img src="/2023/06/01/prml%20EM/6.png" alt><br>其中$\mathbf{x} = (x_1,..x_D)$,$\mathbf{\mu_k} = (\mu_1,..\mu_i..\mu_D)$<br>假如我们有一数据集$\mathbf{X} = (\mathbf{x}_1…\mathbf{x}_N)$，那么我们可以得到这个模型的对数似然函数：<br><img src="/2023/06/01/prml%20EM/7.png" alt><br>可以看到对数内有求和使得最大似然解没有解析解。与高斯分布相同我们引入n个K维的二值隐含变量$z_n$，每一个$z_n$中只有一个特殊的$z_{nk}$等于1其余为0。他来描述第n个数据被分为哪一类，若$z_{nk}= 1$则说明第n个数据$\mathbf{x_n}$属于第k类。$z_n$服从多项式分布。我们使用EM算法。<br>E步骤（求后验）：</p>
<script type="math/tex; mode=display">
p(x_n \vert z_{nk} = 1,\mu) =  p(x_n \vert \mu_k)</script><script type="math/tex; mode=display">
p(z_{nk} = 1) = \pi_k</script><p>自然可以得到后验：</p>
<script type="math/tex; mode=display">
p(z_{nk} = 1)  = \frac{\pi_k p(x_n \vert \mu_k)}{\sum_{j = 1}^K \pi_j p(x_n \vert \mu_j)}</script><p>M步骤（最大化下界）：<br>完整数据的对数似然：</p>
<script type="math/tex; mode=display">
\begin{aligned}
         \ln p(\mathbf{x},\mathbf{z} \vert \pi,\mu) &= \ln \prod_{n=1}^N \prod_{k=1}^K (p(x_n \vert z_{nk})p(z_{nk} ))^{z_{nk}}\\& 
     = \sum_{n=1}^N\sum_{k=1}^K z_{nk} (\ln \pi_k p(x_n \vert \mu_k))\\& = \sum_{n=1}^N\sum_{k=1}^K z_{nk} (\ln \pi_k  +
     \sum_i^D[x_{ni}\ln\mu_{ki}+(1-x_{ni})\ln(1-\mu_{ki})])
\end{aligned}</script><p>下界：</p>
<script type="math/tex; mode=display">\begin{aligned}
    &E_{q(z)}[\sum_{n=1}^N\sum_{k=1}^K z_{nk}(\ln \pi_k  +
     \sum_i^D[x_{ni}\ln\mu_{ki}+(1-x_{ni})\ln(1-\mu_{ki})]) ]
\\ & = \sum_{n=1}^N\sum_{k=1}^K q(z_{nk} = 1) (\ln \pi_k  +
     \sum_i^D[x_{ni}\ln\mu_{ki}+(1-x_{ni})\ln(1-\mu_{ki})])
\end{aligned}</script><p>对各参数求导，或采用拉格朗日乘数法可以得到以下结果。</p>
<script type="math/tex; mode=display">
\mu_k = \frac{1}{N_k}\sum_n q(z_{nk} = 1)\mathbf{x}_n</script><script type="math/tex; mode=display">
\pi_k = \frac{N_k}{N}</script><p>其中：$N = \sum_k^K\sum_n^{N}q(z_{nk} = 1)$，$N_k = \sum_{n= 1}^N q(z_{nk} = 1)$<br>我们可以使用伯努利混合模型对二值化的数字图像进行建模后聚类。其中$x_n$就是各二值化后的图像，得到的潜在变量$z_{nk}$的后验概率则为聚类结果。得到的$\mu_k$则可以看作是各数字的图像的概率分布（就是该算法认为的数字长得样子）。<br><img src="/2023/06/01/prml%20EM/8.png" alt></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">molo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://molo333.github.io/2023/06/01/prml EM/">http://molo333.github.io/2023/06/01/prml EM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="next-post pull-right"><a href="/2023/03/08/prml2.1/"><span>prml第二章笔记上</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2022 - 2023 By molo</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>